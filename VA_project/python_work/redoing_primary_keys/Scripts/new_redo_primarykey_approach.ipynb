{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cayde\\AppData\\Local\\Temp\\ipykernel_4484\\3738703185.py:6: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  email_cell_all_examples = pd.read_csv('../data/CF_CELL_EMAIL_22K_allexamples.csv')\n",
      "C:\\Users\\cayde\\AppData\\Local\\Temp\\ipykernel_4484\\3738703185.py:7: DtypeWarning: Columns (11,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_agr_data_append = pd.read_csv(r'C:\\Users\\cayde\\Desktop\\CF_2_more_orginized\\Projects\\VA_project\\Data\\cf_sampleForAGRtoAppendto_22k_6-18-2022_FINAL RETURN FILE_1 - Copy.csv')\n",
      "C:\\Users\\cayde\\AppData\\Local\\Temp\\ipykernel_4484\\3738703185.py:8: DtypeWarning: Columns (25,26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_appriss_data_append = pd.read_csv('../Data/Cf_appenedToAprrissDataSample_8-18-2022.csv')\n",
      "C:\\Users\\cayde\\AppData\\Local\\Temp\\ipykernel_4484\\3738703185.py:16: DtypeWarning: Columns (15,16,17,18,19,20,21,22,33,35,48,49,50,51,52,53,54,55,56,57,58,59,61,62,63,73,74,75,77,78,79,84,86,87,88,89,90,91,92,96,97,98,107,122,123) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_medial_source = pd.read_csv('../Data/CF_medicalData.csv')\n",
      "C:\\Users\\cayde\\AppData\\Local\\Temp\\ipykernel_4484\\3738703185.py:18: DtypeWarning: Columns (7,8,10,12,13,14,15,16,17,18,19,20,24,25,26,28,29,30,31,32,33,34,38,40,41,42,43,44,45,46,47,48,50,51,52,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_vaprediction_v1 = pd.read_csv('../Data/cf_vaprediction_v1.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "#read in all the data from directory 'all_appends_and_source' and store each in a dataframe \n",
    "df_agr_email_cell = pd.read_csv('../data/CF_CELL_EMAIL_22K_HASEMAIL.csv') # need to decided how we want to go about working with this file\n",
    "email_cell_all_examples = pd.read_csv('../data/CF_CELL_EMAIL_22K_allexamples.csv')\n",
    "df_agr_data_append = pd.read_csv(r'C:\\Users\\cayde\\Desktop\\CF_2_more_orginized\\Projects\\VA_project\\Data\\cf_sampleForAGRtoAppendto_22k_6-18-2022_FINAL RETURN FILE_1 - Copy.csv')\n",
    "df_appriss_data_append = pd.read_csv('../Data/Cf_appenedToAprrissDataSample_8-18-2022.csv')\n",
    "fixed_dev_sample = pd.read_csv('../Data/primary_keys_dev_sample_10_13_2022.csv')\n",
    "old_bad_dev_sample = pd.read_csv('../Data/cf_DataSample_22k_8-8-2022.csv')\n",
    "df_acxiom_append = pd.read_csv('../Data/cf_sampleforAcxiom_22k_6-20-2022_OUT.csv')\n",
    "acxiom_emails = pd.read_csv(r'C:\\Users\\cayde\\Desktop\\CF_2_more_orginized\\Projects\\VA_project\\Data\\cf_sampleforAcxiom_22k_6-20-2022.csv')\n",
    "\n",
    "\n",
    "#use to sanity check\n",
    "df_medial_source = pd.read_csv('../Data/CF_medicalData.csv')\n",
    "df_10krando_source = pd.read_csv('../Data/AGR_OK_18PLUS_DOB_SAMPLE_10000_20220614.csv')\n",
    "df_vaprediction_v1 = pd.read_csv('../Data/cf_vaprediction_v1.csv')\n",
    "\n",
    "sent_to_acxiom = pd.read_csv(r'C:\\Users\\cayde\\Desktop\\CF\\VApredction_v2_data\\cf_sampleforAcxiom_22k_6-20-2022.csv')\n",
    "sent_to_agr = pd.read_csv(r'C:\\Users\\cayde\\Desktop\\CF_2_more_orginized\\Projects\\VA_project\\Data\\cf_sampleForAGRtoAppendto_22k_6-18-2022_1 - Copy.csv')\n",
    "sent_to_appriss = pd.read_csv(r'C:\\Users\\cayde\\Desktop\\CF_2_more_orginized\\Projects\\VA_project\\Data\\cf_DataSample_22k_8-11-2022_cleaned - Copy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the name of column 'ADDRESS_x' to 'ADDRESS' in fixed_dev_sample\n",
    "fixed_dev_sample.rename(columns={'ADDRESS_x':'ADDRESS'}, inplace=True)\n",
    "\n",
    "for_axciom_merge = fixed_dev_sample[['cf_unique_id', 'FIRST_NAME', 'LAST_NAME', 'ADDRESS', 'CITY', 'STATE', 'ZIP']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## acxiom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we ill handle axciom data\n",
    "#merge df_acxiom_append with old_pk_new_pk on cf_id\n",
    "df_acxiom_append = pd.merge(df_acxiom_append, for_axciom_merge, on=['FIRST_NAME', 'LAST_NAME', 'ADDRESS', 'CITY', 'STATE', 'ZIP'], how='left')\n",
    "#move the column cf_unique_id to the front of the df_acxiom_append dataframe by popping it and then inserting it at the front\n",
    "df_acxiom_append.insert(0, 'cf_unique_id', df_acxiom_append.pop('cf_unique_id'))\n",
    "\n",
    "#at df_acxiom_append index 8801 in the column cf_unique_id, insert the value 22204_(medical_source)index_10460\n",
    "df_acxiom_append.at[8801, 'cf_unique_id'] = '22204_(medical_source)index_10460'\n",
    "\n",
    "#at df_acxiom_append index 8801 in the column cf_unique_id, insert the value 22204_(medical_source)index_10460\n",
    "df_acxiom_append.at[11130, 'cf_unique_id'] = '22278_(medical_source)index_11801'\n",
    "\n",
    "#count how many na's are in acxiom data\n",
    "# print(df_acxiom_append['cf_unique_id'].isna().sum())\n",
    "# print(df_acxiom_append['cf_unique_id'].duplicated().sum())\n",
    "#make a dataframe of all rows with duplicate cf_unique_id\n",
    "# df_acxiom_append_dup = df_acxiom_append[df_acxiom_append['cf_unique_id'].duplicated(keep=False)]\n",
    "\n",
    "# #write out the acxiom data to a csv\n",
    "# df_acxiom_append.to_csv('../Data/new_out_data/cf_acxiomAppend_va2_10_14_2022.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frop the column cf_id\n",
    "df_acxiom_append.drop(columns=['cf_id'], inplace=True)\n",
    "\n",
    "#rename the column cf_unique_id to source_id\n",
    "df_acxiom_append.rename(columns={'cf_unique_id':'source_id'}, inplace=True)\n",
    "\n",
    "#add a column called PK and start at 0 and iterate up by 1. make this the first column in dataframe\n",
    "df_acxiom_append.insert(0, 'PK', range(0, len(df_acxiom_append)))\n",
    "\n",
    "# #write out the acxiom data to a csv\n",
    "df_acxiom_append.to_csv('../Data/new_out_data/cf_acxiomAppend_va2_10_14_2022.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #sanity check to make sure data mataches source\n",
    "\n",
    "# #for every first and last name and cf_unique_id in df_acxiom_append\n",
    "# for index, row in df_acxiom_append.iterrows():\n",
    "#     append_fn = row['FIRST_NAME']\n",
    "#     append_ln = row['LAST_NAME']\n",
    "#     #grab the rest of the string after 'index_' and convert it to an int\n",
    "#     append_index = int(row['cf_unique_id'].split('index_')[1])\n",
    "#     #grab the string inbewteen the paranthesis in the cf_unique_id\n",
    "#     source_title = row['cf_unique_id'][row['cf_unique_id'].find('(')+1:row['cf_unique_id'].find(')')]\n",
    "#     if source_title == 'medical_source':\n",
    "#         #grab the row in df_medial_source with the same index as the source_id\n",
    "#         source_row = df_medial_source.loc[int(append_index)]\n",
    "#         #compare the first and last name in the source_row to the first and last name in the append_row\n",
    "#         if source_row['FirstName'] == append_fn and source_row['LastName'] == append_ln:\n",
    "#             #if they are the same continue to the next row if not print the index of the row and the mismatching names\n",
    "#             continue\n",
    "#         else:\n",
    "#             print(index, append_fn, append_ln, source_row['FirstName'], source_row['LastName'])\n",
    "#     elif source_title == '10krando_source':\n",
    "#         #grab the row in df_10krando_source with the same index as the source_id\n",
    "#         source_row = df_10krando_source.loc[int(append_index)]\n",
    "#         #compare the first and last name in the source_row to the first and last name in the append_row\n",
    "#         if source_row['FIRST NAME'] == append_fn and source_row['LAST NAME'] == append_ln:\n",
    "#             #if they are the same continue to the next row if not print the index of the row and the mismatching names\n",
    "#             continue\n",
    "#         else:\n",
    "#             print(index, append_fn, append_ln, source_row['FIRST NAME'], source_row['LAST NAME'])\n",
    "#     elif source_title == 'vaprediction_v1':\n",
    "#         #grab the row in df_vaprediction_v1 with the same index as the source_id\n",
    "#         source_row = df_vaprediction_v1.loc[int(append_index)]\n",
    "#         #compare the first and last name in the source_row to the first and last name in the append_row\n",
    "#         if source_row['FIRST.NAME'] == append_fn and source_row['LAST.NAME'] == append_ln:\n",
    "#             #if they are the same continue to the next row if not print the index of the row and the mismatching names\n",
    "#             continue\n",
    "#         else:\n",
    "#             print(index, append_fn, append_ln, source_row['FIRST.NAME'], source_row['LAST.NAME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agr demographic append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # order the data_frame by the column 'old_index' in ascending order\n",
    "# sent_to_agr.sort_values(by=['index'], inplace=True)\n",
    "# #add a column cf_id to sent_to_agr that starts from 0 to the length of the dataframe\n",
    "# sent_to_agr['cf_id'] = range(0, len(sent_to_agr))\n",
    "\n",
    "for_agr_merge = fixed_dev_sample[['cf_unique_id', 'FIRST_NAME', 'LAST_NAME', 'ADDRESS', 'CITY', 'STATE', \"AGRID16\", \"AGRID15\", \"MASTER_DOB\", \"ZIP\"]]\n",
    "\n",
    "# add a column in sent_to_agr called 'cf_id' that starts at zero and goes to the length of the dataframe\n",
    "sent_to_agr['cf_id'] = range(0, len(sent_to_agr))\n",
    "\n",
    "for_agr_merge = pd.merge(sent_to_agr, for_agr_merge, on=[ 'FIRST_NAME', 'LAST_NAME', 'ADDRESS', 'CITY', 'STATE'], how='left')\n",
    "\n",
    "#count how many na's are in acxiom data\n",
    "print(for_agr_merge['cf_unique_id'].isna().sum())\n",
    "# print out if any cf_unique_id's are duplicated\n",
    "print(for_agr_merge['cf_unique_id'].duplicated().sum())\n",
    "#make a dataframe of all rows with duplicate cf_unique_id\n",
    "df_agr_append_dup = for_agr_merge[for_agr_merge['cf_unique_id'].duplicated(keep=False)]\n",
    "\n",
    "#make all first and last names uppercase\n",
    "for_agr_merge['FIRST_NAME'] = for_agr_merge['FIRST_NAME'].str.upper()\n",
    "for_agr_merge['LAST_NAME'] = for_agr_merge['LAST_NAME'].str.upper()\n",
    "# #remove all special characters from the first and last names\n",
    "# for_agr_merge['FIRST_NAME'] = for_agr_merge['FIRST_NAME'].str.replace('[^A-Za-z0-9]+', '')\n",
    "# for_agr_merge['LAST_NAME'] = for_agr_merge['LAST_NAME'].str.replace('[^A-Za-z0-9]+', '')\n",
    "#remove all punctuation from the first and last names\n",
    "for_agr_merge['FIRST_NAME'] = for_agr_merge['FIRST_NAME'].str.replace('[^\\w\\s]','')\n",
    "for_agr_merge['LAST_NAME'] = for_agr_merge['LAST_NAME'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# for_agr_merge.rename(columns={'FIRST_NAME':'FIRST NAME', 'LAST_NAME':'LAST NAME', 'ADDRESS':'ORIG_ADDRESS', 'CITY':'ORIG_CITY', 'STATE':'ORIG_STATE'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_agr_merge = for_agr_merge[['cf_unique_id', 'index', 'FIRST_NAME', 'LAST_NAME']]\n",
    "\n",
    "#add a column cf_id to sent_to_agr that starts from 0 to the length of the dataframe\n",
    "df_agr_data_append['cf_id'] = range(0, len(df_agr_data_append))\n",
    "\n",
    "df_agr_data_append = pd.merge(df_agr_data_append, for_agr_merge, left_on=['CLIENT_INDEX', 'FIRST NAME', 'LAST NAME'], right_on=['index', 'FIRST_NAME', 'LAST_NAME'], how='left')\n",
    "df_agr_data_append.insert(0, 'cf_unique_id', df_agr_data_append.pop('cf_unique_id'))\n",
    "\n",
    "#count all the rows that have nan in the column cf_unique_id in the agr data\n",
    "print(df_agr_data_append['cf_unique_id'].isna().sum())\n",
    "\n",
    "#at index 2877 set cf_unique_id to 13452_(agr_random_sample)index_1445\n",
    "df_agr_data_append.loc[2877, 'cf_unique_id'] = r'13452_(agr_random_sample)index_1445'\n",
    "df_agr_data_append.loc[11247, 'cf_unique_id'] = r'22064_(medical_source)index_5656'\n",
    "df_agr_data_append.loc[12140, 'cf_unique_id'] = r'22069_(medical_source)index_6105'\n",
    "df_agr_data_append.loc[20416, 'cf_unique_id'] = r'22204_(medical_source)index_10460'\n",
    "df_agr_data_append.loc[21805, 'cf_unique_id'] = r'22278_(medical_source)index_11801'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count all the rows that have nan in the column cf_unique_id in the agr data\n",
    "print(df_agr_data_append['cf_unique_id'].isna().sum())\n",
    "\n",
    "#drop columns 'CLIENT_INDEX', 'cf_id', 'index', 'FIRST_NAME', 'LAST_NAME'\n",
    "df_agr_data_append.drop(columns=['CLIENT_INDEX', 'cf_id', 'index', 'FIRST_NAME', 'LAST_NAME'], inplace=True)\n",
    "\n",
    "#change the name of the column 'cf_unique_id' to 'source_id\n",
    "df_agr_data_append.rename(columns={'cf_unique_id':'source_id'}, inplace=True)\n",
    "\n",
    "#add a column called PK and start at 0 and iterate up by 1. make this the first column in dataframe\n",
    "df_agr_data_append.insert(0, 'PK', range(0, len(df_agr_data_append)))\n",
    "\n",
    "# #write out the agr data to a csv\n",
    "df_agr_data_append.to_csv('../Data/new_out_data/cf_agrAppend_va2_10_14_2022.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check to make sure data mataches source\n",
    "\n",
    "#for every first and last name and cf_unique_id in df_acxiom_append\n",
    "for index, row in df_agr_data_append.iterrows():\n",
    "    append_fn = row['FIRST NAME']\n",
    "    append_ln = row['LAST NAME']\n",
    "    #grab the rest of the string after 'index_' and convert it to an int\n",
    "    append_index = int(row['cf_unique_id'].split('index_')[1])\n",
    "    #grab the string inbewteen the paranthesis in the cf_unique_id\n",
    "    source_title = row['cf_unique_id'][row['cf_unique_id'].find('(')+1:row['cf_unique_id'].find(')')]\n",
    "    if source_title == 'medical_source':\n",
    "        #grab the row in df_medial_source with the same index as the source_id\n",
    "        source_row = df_medial_source.loc[int(append_index)]\n",
    "        #compare the first and last name in the source_row to the first and last name in the append_row\n",
    "        if source_row['FirstName'] == append_fn and source_row['LastName'] == append_ln:\n",
    "            #if they are the same continue to the next row if not print the index of the row and the mismatching names\n",
    "            continue\n",
    "        else:\n",
    "            print(index, append_fn, append_ln, source_row['FirstName'], source_row['LastName'])\n",
    "    elif source_title == '10krando_source':\n",
    "        #grab the row in df_10krando_source with the same index as the source_id\n",
    "        source_row = df_10krando_source.loc[int(append_index)]\n",
    "        #compare the first and last name in the source_row to the first and last name in the append_row\n",
    "        if source_row['FIRST NAME'] == append_fn and source_row['LAST NAME'] == append_ln:\n",
    "            #if they are the same continue to the next row if not print the index of the row and the mismatching names\n",
    "            continue\n",
    "        else:\n",
    "            print(index, append_fn, append_ln, source_row['FIRST NAME'], source_row['LAST NAME'])\n",
    "    elif source_title == 'vaprediction_v1':\n",
    "        #grab the row in df_vaprediction_v1 with the same index as the source_id\n",
    "        source_row = df_vaprediction_v1.loc[int(append_index)]\n",
    "        #compare the first and last name in the source_row to the first and last name in the append_row\n",
    "        if source_row['FIRST.NAME'] == append_fn and source_row['LAST.NAME'] == append_ln:\n",
    "            #if they are the same continue to the next row if not print the index of the row and the mismatching names\n",
    "            continue\n",
    "        else:\n",
    "            print(index, append_fn, append_ln, source_row['FIRST.NAME'], source_row['LAST.NAME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## appriss append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from sent_to_appriss cut down to only the columns we need\n",
    "# for_appriss_merge = sent_to_appriss[['Client Billing ID', 'First Name', 'Last Name']]\n",
    "# #rename the columns to match the columns in the append data\n",
    "# for_appriss_merge.rename(columns={'Client Billing ID':'cf_id', 'First Name':'FIRST NAME', 'Last Name':'LAST NAME'}, inplace=True)\n",
    "# #merge the append data with the fixed_dev_sample\n",
    "# for_appriss_merge = pd.merge(for_appriss_merge, fixed_dev_sample, left_on=['FIRST NAME', 'LAST NAME'], right_on=['FIRST NAME', 'LAST NAME'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #merge df_appriss_data_append with old_pk_new_pk left on ClientBillingID and right on cf_id\n",
    "# df_appriss_data_append = pd.merge(df_appriss_data_append, for_agr_merge, left_on='ClientBillingID', right_on='cf_id', how='left')\n",
    "# df_appriss_data_append.insert(0, 'cf_unique_id', df_appriss_data_append.pop('cf_unique_id'))\n",
    "\n",
    "# # #drop the column ClientBillingID\n",
    "# df_appriss_data_append = df_appriss_data_append.drop(columns=['ClientBillingID'])\n",
    "\n",
    "# #writing out the appriss data to a csv\n",
    "# df_appriss_data_append.to_csv('../Data/new_out_data/cf_apprissAppend_va2_10_14_2022.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data from the csv\n",
    "df_appriss_data_append = pd.read_csv('../Data/new_out_data/cf_apprissAppend_va2_10_14_2022.csv')\n",
    "\n",
    "# drop the coulmn 'cf_id\n",
    "df_appriss_data_append.drop(columns=['cf_id'], inplace=True)\n",
    "\n",
    "#change the name of the column 'cf_unique_id' to 'source_id\n",
    "df_appriss_data_append.rename(columns={'cf_unique_id':'source_id'}, inplace=True)\n",
    "\n",
    "#add a column called PK and start at 0 and iterate up by 1. make this the first column in dataframe\n",
    "df_appriss_data_append.insert(0, 'PK', range(0, len(df_appriss_data_append)))\n",
    "\n",
    "# #write out the appriss data to a csv\n",
    "df_appriss_data_append.to_csv('../Data/new_out_data/cf_apprissAppend_va2_10_17_2022.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# #read in data from the csv\n",
    "# df_appriss_data_append = pd.read_csv('../Data/new_out_data/cf_apprissAppend_va2_10_14_2022.csv')\n",
    "\n",
    "#for every first and last name and cf_unique_id in df_acxiom_append\n",
    "for index, row in df_appriss_data_append.iterrows():\n",
    "    #grab the value of column PersonFirstName and remove []\n",
    "    append_fn = row['PersonFirstName'].replace('[', '').replace(']', '')\n",
    "    #grab the value of column PersonLastName and remove []\n",
    "    append_ln = row['PersonLastName'].replace('[', '').replace(']', '')\n",
    "    #grab the rest of the string after 'index_' and convert it to an int\n",
    "    append_index = int(row['cf_unique_id'].split('index_')[1])\n",
    "    #grab the string inbewteen the paranthesis in the cf_unique_id\n",
    "    source_title = row['cf_unique_id'][row['cf_unique_id'].find('(')+1:row['cf_unique_id'].find(')')]\n",
    "    if source_title == 'medical_source':\n",
    "        #grab the row in df_medial_source with the same index as the source_id\n",
    "        source_row = df_medial_source.loc[int(append_index)]\n",
    "        #compare the first and last name in the source_row to the first and last name in the append_row\n",
    "        if source_row['FirstName'] == append_fn and source_row['LastName'] == append_ln:\n",
    "            #if they are the same continue to the next row if not print the index of the row and the mismatching names\n",
    "            continue\n",
    "        else:\n",
    "            print(index, append_fn, append_ln, source_row['FirstName'], source_row['LastName'])\n",
    "    elif source_title == '10krando_source':\n",
    "        #grab the row in df_10krando_source with the same index as the source_id\n",
    "        source_row = df_10krando_source.loc[int(append_index)]\n",
    "        #compare the first and last name in the source_row to the first and last name in the append_row\n",
    "        if source_row['FIRST NAME'] == append_fn and source_row['LAST NAME'] == append_ln:\n",
    "            #if they are the same continue to the next row if not print the index of the row and the mismatching names\n",
    "            continue\n",
    "        else:\n",
    "            print(index, append_fn, append_ln, source_row['FIRST NAME'], source_row['LAST NAME'])\n",
    "    elif source_title == 'vaprediction_v1':\n",
    "        #grab the row in df_vaprediction_v1 with the same index as the source_id\n",
    "        source_row = df_vaprediction_v1.loc[int(append_index)]\n",
    "        #compare the first and last name in the source_row to the first and last name in the append_row\n",
    "        if source_row['FIRST.NAME'] == append_fn and source_row['LAST.NAME'] == append_ln:\n",
    "            #if they are the same continue to the next row if not print the index of the row and the mismatching names\n",
    "            continue\n",
    "        else:\n",
    "            print(index, append_fn, append_ln, source_row['FIRST.NAME'], source_row['LAST.NAME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agr email cell append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # order the data_frame by the column 'old_index' in ascending order\n",
    "# sent_to_agr.sort_values(by=['index'], inplace=True)\n",
    "# #add a column cf_id to sent_to_agr that starts from 0 to the length of the dataframe\n",
    "# sent_to_agr['cf_id'] = range(0, len(sent_to_agr))\n",
    "\n",
    "for_agr_merge = fixed_dev_sample[['cf_unique_id', 'FIRST_NAME', 'LAST_NAME', 'ADDRESS', 'CITY', 'STATE', \"AGRID16\", \"AGRID15\", \"MASTER_DOB\", \"ZIP\"]]\n",
    "\n",
    "# add a column in sent_to_agr called 'cf_id' that starts at zero and goes to the length of the dataframe\n",
    "sent_to_agr['cf_id'] = range(0, len(sent_to_agr))\n",
    "\n",
    "for_agr_merge = pd.merge(sent_to_agr, for_agr_merge, on=[ 'FIRST_NAME', 'LAST_NAME', 'ADDRESS', 'CITY', 'STATE'], how='left')\n",
    "\n",
    "#count how many na's are in acxiom data\n",
    "print(for_agr_merge['cf_unique_id'].isna().sum())\n",
    "# print out if any cf_unique_id's are duplicated\n",
    "print(for_agr_merge['cf_unique_id'].duplicated().sum())\n",
    "#make a dataframe of all rows with duplicate cf_unique_id\n",
    "df_agr_append_dup = for_agr_merge[for_agr_merge['cf_unique_id'].duplicated(keep=False)]\n",
    "\n",
    "#creat a column called unotched_firstname that is the same as the column FIRST_NAME\n",
    "for_agr_merge['unotched_firstname'] = for_agr_merge['FIRST_NAME']\n",
    "#creat a column called unotched_lastname that is the same as the column LAST_NAME\n",
    "for_agr_merge['unotched_lastname'] = for_agr_merge['LAST_NAME']\n",
    "\n",
    "#make all first and last names uppercase\n",
    "for_agr_merge['FIRST_NAME'] = for_agr_merge['FIRST_NAME'].str.upper()\n",
    "for_agr_merge['LAST_NAME'] = for_agr_merge['LAST_NAME'].str.upper()\n",
    "# #remove all special characters from the first and last names\n",
    "# for_agr_merge['FIRST_NAME'] = for_agr_merge['FIRST_NAME'].str.replace('[^A-Za-z0-9]+', '')\n",
    "# for_agr_merge['LAST_NAME'] = for_agr_merge['LAST_NAME'].str.replace('[^A-Za-z0-9]+', '')\n",
    "#remove all punctuation from the first and last names\n",
    "for_agr_merge['FIRST_NAME'] = for_agr_merge['FIRST_NAME'].str.replace('[^\\w\\s]','')\n",
    "for_agr_merge['LAST_NAME'] = for_agr_merge['LAST_NAME'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# for_agr_merge.rename(columns={'FIRST_NAME':'FIRST NAME', 'LAST_NAME':'LAST NAME', 'ADDRESS':'ORIG_ADDRESS', 'CITY':'ORIG_CITY', 'STATE':'ORIG_STATE'}, inplace=True)\n",
    "\n",
    "for_agr_merge = for_agr_merge[['cf_unique_id', 'index', 'FIRST_NAME', 'LAST_NAME', 'unotched_firstname', 'unotched_lastname']]\n",
    "\n",
    "#add a column cf_id to sent_to_agr that starts from 0 to the length of the dataframe\n",
    "df_agr_data_append['cf_id'] = range(0, len(df_agr_data_append))\n",
    "\n",
    "df_agr_data_append = pd.merge(df_agr_data_append, for_agr_merge, left_on=['CLIENT_INDEX', 'FIRST NAME', 'LAST NAME'], right_on=['index', 'FIRST_NAME', 'LAST_NAME'], how='left')\n",
    "df_agr_data_append.insert(0, 'cf_unique_id', df_agr_data_append.pop('cf_unique_id'))\n",
    "\n",
    "#at index 2877 set cf_unique_id to 13452_(agr_random_sample)index_1445\n",
    "df_agr_data_append.loc[2877, 'cf_unique_id'] = r'13452_(agr_random_sample)index_1445'\n",
    "df_agr_data_append.loc[11247, 'cf_unique_id'] = r'22064_(medical_source)index_5656'\n",
    "df_agr_data_append.loc[12140, 'cf_unique_id'] = r'22069_(medical_source)index_6105'\n",
    "df_agr_data_append.loc[20416, 'cf_unique_id'] = r'22204_(medical_source)index_10460'\n",
    "df_agr_data_append.loc[21805, 'cf_unique_id'] = r'22278_(medical_source)index_11801'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_cell_email_merge = df_agr_data_append[['cf_unique_id', 'FIRST NAME', 'LAST NAME', 'ADDRESS', 'CITY', 'STATE']]\n",
    "email_cell_all_examples = pd.merge(email_cell_all_examples, for_cell_email_merge, left_on=['FIRST NAME', 'LAST NAME', 'ORIG_ADDRESS', 'ORIG_CITY', 'ORIG_STATE'], right_on=['FIRST NAME', 'LAST NAME', 'ADDRESS', 'CITY', 'STATE'], how='left')\n",
    "email_cell_all_examples.insert(0, 'cf_unique_id', email_cell_all_examples.pop('cf_unique_id'))\n",
    "email_cell_all_examples = email_cell_all_examples.drop(columns=['CF_ID'])\n",
    "\n",
    "# drop address_y, city_y, state_y columns\n",
    "email_cell_all_examples = email_cell_all_examples.drop(columns=['ADDRESS_y', 'CITY_y', 'STATE_y'])\n",
    "\n",
    "# rename address_x, city_x, state_x columns to address, city, state\n",
    "email_cell_all_examples.rename(columns={'ADDRESS_x':'ADDRESS', 'CITY_x':'CITY', 'STATE_x':'STATE'}, inplace=True)\n",
    "\n",
    "# rename cf_unique id to source_id \n",
    "email_cell_all_examples.rename(columns={'cf_unique_id':'source_id'}, inplace=True)\n",
    "\n",
    "# add a coulmn called 'PK' and itterate from 0 to the length of the dataframe\n",
    "email_cell_all_examples['PK'] = range(0, len(email_cell_all_examples))\n",
    "\n",
    "#make pk the first row in the dataframe\n",
    "email_cell_all_examples.insert(0, 'PK', email_cell_all_examples.pop('PK'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing out the agr email cell data to a csv\n",
    "email_cell_all_examples.to_csv('../Data/new_out_data/cf_agrEmailCellAppend_va2_10_14_2022.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conut how many na's are in the cf_unique_id column\n",
    "print(email_cell_all_examples['cf_unique_id'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #read in data from the csv\n",
    "# df_agr_email_cell = pd.read_csv('../Data/new_out_data/cf_agrEmailCellAppend_va2_10_14_2022.csv')\n",
    "\n",
    "#for every first and last name and cf_unique_id in df_acxiom_append\n",
    "for index, row in email_cell_all_examples.iterrows():\n",
    "    append_fn = row['FIRST NAME']\n",
    "    append_ln = row['LAST NAME']\n",
    "    #grab the rest of the string after 'index_' and convert it to an int\n",
    "    append_index = int(row['cf_unique_id'].split('index_')[1])\n",
    "    #grab the string inbewteen the paranthesis in the cf_unique_id\n",
    "    source_title = row['cf_unique_id'][row['cf_unique_id'].find('(')+1:row['cf_unique_id'].find(')')]\n",
    "    if source_title == 'medical_source':\n",
    "        #grab the row in df_medial_source with the same index as the source_id\n",
    "        source_row = df_medial_source.loc[int(append_index)]\n",
    "        #compare the first and last name in the source_row to the first and last name in the append_row\n",
    "        if source_row['FirstName'] == append_fn and source_row['LastName'] == append_ln:\n",
    "            #if they are the same continue to the next row if not print the index of the row and the mismatching names\n",
    "            continue\n",
    "        else:\n",
    "            print(index, append_fn, append_ln, source_row['FirstName'], source_row['LastName'])\n",
    "    elif source_title == '10krando_source':\n",
    "        #grab the row in df_10krando_source with the same index as the source_id\n",
    "        source_row = df_10krando_source.loc[int(append_index)]\n",
    "        #compare the first and last name in the source_row to the first and last name in the append_row\n",
    "        if source_row['FIRST NAME'] == append_fn and source_row['LAST NAME'] == append_ln:\n",
    "            #if they are the same continue to the next row if not print the index of the row and the mismatching names\n",
    "            continue\n",
    "        else:\n",
    "            print(index, append_fn, append_ln, source_row['FIRST NAME'], source_row['LAST NAME'])\n",
    "    elif source_title == 'vaprediction_v1':\n",
    "        #grab the row in df_vaprediction_v1 with the same index as the source_id\n",
    "        source_row = df_vaprediction_v1.loc[int(append_index)]\n",
    "        #compare the first and last name in the source_row to the first and last name in the append_row\n",
    "        if source_row['FIRST.NAME'] == append_fn and source_row['LAST.NAME'] == append_ln:\n",
    "            #if they are the same continue to the next row if not print the index of the row and the mismatching names\n",
    "            continue\n",
    "        else:\n",
    "            print(index, append_fn, append_ln, source_row['FIRST.NAME'], source_row['LAST.NAME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## axciom emails "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we ill handle axciom data\n",
    "#merge df_acxiom_append with old_pk_new_pk on cf_id\n",
    "acxiom_emails = pd.merge(acxiom_emails, for_axciom_merge, on=['FIRST_NAME', 'LAST_NAME', 'ADDRESS', 'CITY', 'STATE', 'ZIP'], how='left')\n",
    "#move the column cf_unique_id to the front of the df_acxiom_append dataframe by popping it and then inserting it at the front\n",
    "acxiom_emails.insert(0, 'cf_unique_id', acxiom_emails.pop('cf_unique_id'))\n",
    "\n",
    "#at df_acxiom_append index 8801 in the column cf_unique_id, insert the value 22204_(medical_source)index_10460\n",
    "acxiom_emails.at[3386, 'cf_unique_id'] = '22204_(medical_source)index_10460'\n",
    "\n",
    "#at df_acxiom_append index 8801 in the column cf_unique_id, insert the value 22204_(medical_source)index_10460\n",
    "acxiom_emails.at[7590, 'cf_unique_id'] = '22278_(medical_source)index_11801'\n",
    "\n",
    "#count how many na's are in acxiom data\n",
    "# print(df_acxiom_append['cf_unique_id'].isna().sum())\n",
    "# print(df_acxiom_append['cf_unique_id'].duplicated().sum())\n",
    "#make a dataframe of all rows with duplicate cf_unique_id\n",
    "# df_acxiom_append_dup = df_acxiom_append[df_acxiom_append['cf_unique_id'].duplicated(keep=False)]\n",
    "\n",
    "# #write out the acxiom data to a csv\n",
    "# df_acxiom_append.to_csv('../Data/new_out_data/cf_acxiomAppend_va2_10_14_2022.csv', index=False)\n",
    "\n",
    "# frop the column cf_id\n",
    "acxiom_emails.drop(columns=['cf_id'], inplace=True)\n",
    "\n",
    "#rename the column cf_unique_id to source_id\n",
    "acxiom_emails.rename(columns={'cf_unique_id':'source_id'}, inplace=True)\n",
    "\n",
    "#add a column called PK and start at 0 and iterate up by 1. make this the first column in dataframe\n",
    "acxiom_emails.insert(0, 'PK', range(0, len(acxiom_emails)))\n",
    "\n",
    "# #write out the acxiom data to a csv\n",
    "acxiom_emails.to_csv('../Data/new_out_data/cf_acxiomEmails_va2_10_17_2022.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check to make sure data mataches source\n",
    "\n",
    "#for every first and last name and cf_unique_id in df_acxiom_append\n",
    "for index, row in acxiom_emails.iterrows():\n",
    "    append_fn = row['FIRST_NAME']\n",
    "    append_ln = row['LAST_NAME']\n",
    "    #grab the rest of the string after 'index_' and convert it to an int\n",
    "    append_index = int(row['source_id'].split('index_')[1])\n",
    "    #grab the string inbewteen the paranthesis in the cf_unique_id\n",
    "    source_title = row['source_id'][row['source_id'].find('(')+1:row['source_id'].find(')')]\n",
    "    if source_title == 'medical_source':\n",
    "        #grab the row in df_medial_source with the same index as the source_id\n",
    "        source_row = df_medial_source.loc[int(append_index)]\n",
    "        #compare the first and last name in the source_row to the first and last name in the append_row\n",
    "        if source_row['FirstName'] == append_fn and source_row['LastName'] == append_ln:\n",
    "            #if they are the same continue to the next row if not print the index of the row and the mismatching names\n",
    "            continue\n",
    "        else:\n",
    "            print(index, append_fn, append_ln, source_row['FirstName'], source_row['LastName'])\n",
    "    elif source_title == '10krando_source':\n",
    "        #grab the row in df_10krando_source with the same index as the source_id\n",
    "        source_row = df_10krando_source.loc[int(append_index)]\n",
    "        #compare the first and last name in the source_row to the first and last name in the append_row\n",
    "        if source_row['FIRST NAME'] == append_fn and source_row['LAST NAME'] == append_ln:\n",
    "            #if they are the same continue to the next row if not print the index of the row and the mismatching names\n",
    "            continue\n",
    "        else:\n",
    "            print(index, append_fn, append_ln, source_row['FIRST NAME'], source_row['LAST NAME'])\n",
    "    elif source_title == 'vaprediction_v1':\n",
    "        #grab the row in df_vaprediction_v1 with the same index as the source_id\n",
    "        source_row = df_vaprediction_v1.loc[int(append_index)]\n",
    "        #compare the first and last name in the source_row to the first and last name in the append_row\n",
    "        if source_row['FIRST.NAME'] == append_fn and source_row['LAST.NAME'] == append_ln:\n",
    "            #if they are the same continue to the next row if not print the index of the row and the mismatching names\n",
    "            continue\n",
    "        else:\n",
    "            print(index, append_fn, append_ln, source_row['FIRST.NAME'], source_row['LAST.NAME'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1ebc6ce21e20a44ca2e939d468f8c4914f49f277b8be916293753b9e7a9feef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
